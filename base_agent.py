import os
from openai import OpenAI
from dotenv import load_dotenv
from typing import List, Dict

# åŠ è½½ .env æ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡
load_dotenv()

class HelloAgentsLLM:
    """
    ä¸ºæœ¬ä¹¦ "Hello Agents" å®šåˆ¶çš„LLMå®¢æˆ·ç«¯ã€‚
    å®ƒç”¨äºè°ƒç”¨ä»»ä½•å…¼å®¹OpenAIæ¥å£çš„æœåŠ¡ï¼Œå¹¶é»˜è®¤ä½¿ç”¨æµå¼å“åº”ã€‚
    """
    def __init__(self, model: str = None, apiKey: str = None, baseUrl: str = None, timeout: int = None):
        """
        åˆå§‹åŒ–å®¢æˆ·ç«¯ã€‚ä¼˜å…ˆä½¿ç”¨ä¼ å…¥å‚æ•°ï¼Œå¦‚æœæœªæä¾›ï¼Œåˆ™ä»ç¯å¢ƒå˜é‡åŠ è½½ã€‚
        """
        self.model = model or os.getenv("LLM_MODEL_ID")
        apiKey = apiKey or os.getenv("LLM_API_KEY")
        baseUrl = baseUrl or os.getenv("LLM_BASE_URL")
        timeout = timeout or int(os.getenv("LLM_TIMEOUT", 60))
        
        if not all([self.model, apiKey, baseUrl]):
            raise ValueError("æ¨¡å‹IDã€APIå¯†é’¥å’ŒæœåŠ¡åœ°å€å¿…é¡»è¢«æä¾›æˆ–åœ¨.envæ–‡ä»¶ä¸­å®šä¹‰ã€‚")
        # æ„å»ºäº†self.client å³openaiçš„å®¢æˆ·ç«¯
        self.client = OpenAI(api_key=apiKey, base_url=baseUrl, timeout=timeout)

    def think(self, messages: List[Dict[str, str]], temperature: float = 0) -> str:
        """
        think() â€”â€”â€”â€”å‘å¤§è¯­è¨€æ¨¡å‹å‘é€æ¶ˆæ¯å¹¶è·å–å“åº”
        æµç¨‹ï¼š
            1. æ‰“å°å¼€å§‹è°ƒç”¨æç¤º
            2. åˆ›å»ºæµå¼ API è¯·æ±‚
            3. è¿­ä»£å¤„ç†æ¯ä¸ªå“åº”å—
            4. æ”¶é›†å¹¶è¿”å›å®Œæ•´å“åº”
                4.1 å®æ—¶æ˜¾ç¤ºï¼šé€å—æ˜¾ç¤ºç”Ÿæˆå†…å®¹
                4.2 å†…å­˜é«˜æ•ˆï¼šæ— éœ€ç­‰å¾…å®Œæ•´å“åº”å³å¯å¼€å§‹å¤„ç†
                4.3 ç”¨æˆ·ä½“éªŒå¥½ï¼šæä¾›å®æ—¶åé¦ˆ
            5. å¤„ç†å¼‚å¸¸æƒ…å†µ
        """
        print(f"ğŸ§  æ­£åœ¨è°ƒç”¨ {self.model} æ¨¡å‹...")
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=temperature,
                stream=True, # stream æ˜¯ Generatorï¼Œè¿”å›çš„æ¯ä¸ªchunkæ˜¯ ChatCompletionChunk, è‹¥ä¸ºFalse,åˆ™ä¸ºä¸€æ¬¡æ€§å®Œæ•´è¾“å‡º
            )
            
            # å¤„ç†æµå¼å“åº”
            print("âœ… å¤§è¯­è¨€æ¨¡å‹å“åº”æˆåŠŸ:")
            collected_content = []
             # æ¯æ¬¡è¿­ä»£è·å–ä¸€ä¸ªæ•°æ®å—
            for chunk in response:  
                # ä¸ºä»€ä¹ˆéœ€è¦ or ""ï¼Ÿ  â€”â€”â€”â€”æœ‰äº› chunk åªåŒ…å« metadataï¼ˆå¦‚ roleã€finish_reasonï¼‰ï¼Œæ²¡æœ‰ content
                content = chunk.choices[0].delta.content or ""
                print(content, end="", flush=True)
                collected_content.append(content)    # O(1) æ“ä½œ
            print()  # åœ¨æµå¼è¾“å‡ºç»“æŸåæ¢è¡Œ
            return "".join(collected_content)        # ä¸€æ¬¡æ€§æ‹¼æ¥ï¼ŒO(n) æ•ˆç‡

        except Exception as e:
            print(f"âŒ è°ƒç”¨LLM APIæ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return None         # è¿”å›å®‰å…¨å€¼è€Œä¸æ˜¯å´©æºƒ

# --- å®¢æˆ·ç«¯ä½¿ç”¨ç¤ºä¾‹ ---
if __name__ == '__main__':
    try:
        llmClient = HelloAgentsLLM()
        
        exampleMessages = [
            {"role": "system", "content": "You are a helpful assistant that writes Python code."},
            {"role": "user", "content": "å†™ä¸€ä¸ªä¸‰æ•°ä¹‹å’Œï¼Œéå¸¸ç®€å•çš„å›ç­”å°±è¡Œ"}
        ]
        
        print("--- è°ƒç”¨LLM ---")
        responseText = llmClient.think(exampleMessages)
        if responseText:
            print("\n\n--- å®Œæ•´æ¨¡å‹å“åº” ---")
            print(responseText)

    except ValueError as e:
        print(e)



